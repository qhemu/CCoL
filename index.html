<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <!-- Primary Meta Tags -->
  <!-- TODO: Replace with your paper title and author names -->
  <meta name="title" content="PAPER_TITLE - AUTHOR_NAMES">
  <!-- TODO: Write a compelling 150-160 character description of your research -->
  <meta name="description" content="BRIEF_DESCRIPTION_OF_YOUR_RESEARCH_CONTRIBUTION_AND_FINDINGS">
  <!-- TODO: Add 5-10 relevant keywords for your research area -->
  <meta name="keywords" content="KEYWORD1, KEYWORD2, KEYWORD3, machine learning, computer vision, AI">
  <!-- TODO: List all authors -->
  <meta name="author" content="FIRST_AUTHOR_NAME, SECOND_AUTHOR_NAME">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">
  
  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <!-- TODO: Replace with your institution or lab name -->
  <meta property="og:site_name" content="INSTITUTION_OR_LAB_NAME">
  <!-- TODO: Same as paper title above -->
  <meta property="og:title" content="PAPER_TITLE">
  <!-- TODO: Same as description above -->
  <meta property="og:description" content="BRIEF_DESCRIPTION_OF_YOUR_RESEARCH_CONTRIBUTION_AND_FINDINGS">
  <!-- TODO: Replace with your actual website URL -->
  <meta property="og:url" content="https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE">
  <!-- TODO: Create a 1200x630px preview image and update path -->
  <meta property="og:image" content="https://YOUR_DOMAIN.com/static/images/social_preview.png">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta property="og:image:alt" content="PAPER_TITLE - Research Preview">
  <meta property="article:published_time" content="2024-01-01T00:00:00.000Z">
  <meta property="article:author" content="FIRST_AUTHOR_NAME">
  <meta property="article:section" content="Research">
  <meta property="article:tag" content="KEYWORD1">
  <meta property="article:tag" content="KEYWORD2">

  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image">
  <!-- TODO: Replace with your lab/institution Twitter handle -->
  <meta name="twitter:site" content="@YOUR_TWITTER_HANDLE">
  <!-- TODO: Replace with first author's Twitter handle -->
  <meta name="twitter:creator" content="@AUTHOR_TWITTER_HANDLE">
  <!-- TODO: Same as paper title above -->
  <meta name="twitter:title" content="PAPER_TITLE">
  <!-- TODO: Same as description above -->
  <meta name="twitter:description" content="BRIEF_DESCRIPTION_OF_YOUR_RESEARCH_CONTRIBUTION_AND_FINDINGS">
  <!-- TODO: Same as social preview image above -->
  <meta name="twitter:image" content="https://YOUR_DOMAIN.com/static/images/social_preview.png">
  <meta name="twitter:image:alt" content="PAPER_TITLE - Research Preview">

  <!-- Academic/Research Specific -->
  <meta name="citation_title" content="PAPER_TITLE">
  <meta name="citation_author" content="FIRST_AUTHOR_LAST, FIRST_AUTHOR_FIRST">
  <meta name="citation_author" content="SECOND_AUTHOR_LAST, SECOND_AUTHOR_FIRST">
  <meta name="citation_publication_date" content="2024">
  <meta name="citation_conference_title" content="CONFERENCE_NAME">
  <meta name="citation_pdf_url" content="https://YOUR_DOMAIN.com/static/pdfs/paper.pdf">
  
  <!-- Additional SEO -->
  <meta name="theme-color" content="#2563eb">
  <meta name="msapplication-TileColor" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">
  
  <!-- Preconnect for performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://documentcloud.adobe.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">


  <!-- TODO: Replace with your paper title and authors -->
  <title>PAPER_TITLE - AUTHOR_NAMES | Academic Research</title>
  
  <!-- Favicon and App Icons -->
  <!-- <link rel="icon" type="image/x-icon" href="static/images/robot.svg"> -->
  <!-- <link rel="apple-touch-icon" href="static/images/robot.svg"> -->
  <link rel="icon" type="image/png" href="static/images/robot.png">
  <link rel="apple-touch-icon" href="static/images/robot.png">
  
  <!-- Critical CSS - Load synchronously -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  
  <!-- Non-critical CSS - Load asynchronously -->
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  
  <!-- Fallback for browsers that don't support preload -->
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>
  
  <!-- Fonts - Optimized loading -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
  
  <!-- Defer non-critical JavaScript -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>
  
  <!-- Structured Data for Academic Papers -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "PAPER_TITLE",
    "description": "BRIEF_DESCRIPTION_OF_YOUR_RESEARCH_CONTRIBUTION_AND_FINDINGS",
    "author": [
      {
        "@type": "Person",
        "name": "FIRST_AUTHOR_NAME",
        "affiliation": {
          "@type": "Organization",
          "name": "INSTITUTION_NAME"
        }
      },
      {
        "@type": "Person",
        "name": "SECOND_AUTHOR_NAME",
        "affiliation": {
          "@type": "Organization",
          "name": "INSTITUTION_NAME"
        }
      }
    ],
    "datePublished": "2024-01-01",
    "publisher": {
      "@type": "Organization",
      "name": "CONFERENCE_OR_JOURNAL_NAME"
    },
    "url": "https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE",
    "image": "https://YOUR_DOMAIN.com/static/images/social_preview.png",
    "keywords": ["KEYWORD1", "KEYWORD2", "KEYWORD3", "machine learning", "computer vision"],
    "abstract": "FULL_ABSTRACT_TEXT_HERE",
    "citation": "BIBTEX_CITATION_HERE",
    "isAccessibleForFree": true,
    "license": "https://creativecommons.org/licenses/by/4.0/",
    "mainEntity": {
      "@type": "WebPage",
      "@id": "https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE"
    },
    "about": [
      {
        "@type": "Thing",
        "name": "RESEARCH_AREA_1"
      },
      {
        "@type": "Thing", 
        "name": "RESEARCH_AREA_2"
      }
    ]
  }
  </script>
  
  <script>
document.addEventListener('DOMContentLoaded', () => {

  const tabs = document.querySelectorAll('#video-tabs li');

  const tabContents = document.querySelectorAll('.tab-content');

  tabs.forEach(tab => {
    tab.addEventListener('click', () => {

      tabs.forEach(item => item.classList.remove('is-active'));
      

      tabContents.forEach(content => {
        content.style.display = 'none'; 
      });

      tab.classList.add('is-active');

      const targetContentId = tab.getAttribute('data-tab') + '-content';
      const targetContent = document.getElementById(targetContentId);
      if (targetContent) {
        targetContent.style.display = 'block'; 
      }
    });
  });
});
</script>

  <!-- Website/Organization Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "name": "INSTITUTION_OR_LAB_NAME",
    "url": "https://YOUR_INSTITUTION_WEBSITE.com",
    "logo": "https://YOUR_DOMAIN.com/static/images/favicon.ico",
    "sameAs": [
      "https://twitter.com/YOUR_TWITTER_HANDLE",
      "https://github.com/YOUR_GITHUB_USERNAME"
    ]
  }
  </script>
</head>
<body>


  <!-- Scroll to Top Button -->
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>


  <main id="main-content">
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-widescreen">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <!-- TODO: Replace with your paper title -->
            <h1 class="title is-1 publication-title" style="
    font-size: 2.3em;
    color: #333;
    text-align: center;
    margin-top: 40px;
    margin-bottom: 10px;
    display: inline-block;
    transition: transform 0.2s ease;" >Continuous Vision-Language-Action Co-Learning with Semantic-Physical Alignment for Behavioral Cloning</h1>
             <span class="author-block" style="    text-align: center;
                        font-size: 1.3em;
                        font-weight: bold;
                        color: #900028;
                        margin-bottom: 20px;
                        display: inline-block;
                        transition: transform 0.2s 
                    ease;">
                      Accepted to AAAI Conference on Artificial Intelligence (AAAI 2026)
                    </span>
            <div class="is-size-5 publication-authors">
              <!-- <span class="author-block"><a href="https://qhemu.github.io/" target="_blank">Xiuxiu Qi</a><sup>1,2</sup>,</span>
              <span class="author-block"><a href="https://pappl.eduhk.hk/rich/web/person.xhtml?pid=316524&name=YANG-Yu" target="_blank">Yu Yang</a><sup>3</sup>,</span>
              <span class="author-block"><a href="https://www4.comp.polyu.edu.hk/~csjcao/" target="_blank">Jiannong Cao</a><sup>2</sup>,</span>
              <span class="author-block"><a href="https://haku-pages.notion.site/Luyao-Bai-927ef0e597ef4b25bf56c6c82ca497b9" target="_blank">Luyao Bai</a><sup>2</sup>,</span>
              <span class="author-block"><a href="[作者3的主页链接]" target="_blank">Chongshan Fan</a><sup>1</sup>,</span>
              <span class="author-block"><a href="https://chengtaicao.github.io/" target="_blank">Chengtai Cao</a><sup>4</sup>,</span>
              <span class="author-block"><a href="https://farlab.nankai.edu.cn/FACULTY_STAFFS.htm" target="_blank">Hongpeng Wang</a><sup>1</sup></span> -->
              <a href="https://qhemu.github.io/" target="_blank">Xiuxiu Qi</a><sup>1,2</sup>,
              <a href="https://pappl.eduhk.hk/rich/web/person.xhtml?pid=316524&name=YANG-Yu" target="_blank">Yu Yang</a><sup>3</sup>,
              <a href="https://www4.comp.polyu.edu.hk/~csjcao/" target="_blank">Jiannong Cao</a><sup>2</sup>,
              <a href="https://haku-pages.notion.site/Luyao-Bai-927ef0e597ef4b25bf56c6c82ca497b9" target="_blank">Luyao Bai</a><sup>2</sup>,
              <a href="[作者3的主页链接]" target="_blank">Chongshan Fan</a><sup>1</sup>,
              <a href="https://chengtaicao.github.io/" target="_blank">Chengtai Cao</a><sup>4</sup>,
              <a href="https://farlab.nankai.edu.cn/FACULTY_STAFFS.htm" target="_blank">Hongpeng Wang</a><sup>1</sup>
            </div>

                 <div class="is-size-5 publication-authors">
                   <sup>1</sup>Nankai University, <sup>2</sup>The Hong Kong Polytechnic University, </div>
                <div class="is-size-5 publication-authors">
                  <sup>3</sup>The Education University of Hong Kong, <sup>4</sup> City University of Hong Kong</div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- TODO: Update with your arXiv paper ID -->
                      <span class="link-block">
                        <a href="#" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- TODO: Add your supplementary material PDF or remove this section -->
                    <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>

                  <!-- TODO: Replace with your GitHub repository URL -->
                  <span class="link-block">
                    <a href="#" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- TODO: Update with your arXiv paper ID -->
                <span class="link-block">
                  <a href="#" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>


              <span class="link-block">
                    <a href="https://www.youtube.com/playlist?list=PLtjw6l_L2f0tjHrPQ5iEH3f5lByaVt5qd" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-youtube"></i>
                    </span>
                    <span>All Videos</span>
                  </a>
                </span>
                      
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-widescreen">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <!-- TODO: Replace with your paper abstract -->
          <p>
            Language-conditioned manipulation facilitates human-robot interaction via behavioral cloning (BC), which learns control policies from human demonstrations and serves as a cornerstone of embodied AI. Overcoming compounding errors in sequential action decisions remains a central challenge to improving BC performance. Existing approaches mitigate compounding errors through data augmentation, expressive representation, or temporal abstraction. However, they suffer from physical discontinuities and semantic-physical misalignment, leading to inaccurate action cloning and intermittent execution. In this paper, we present Continuous vision-language-action Co-Learning with Semantic-Physical Alignment (CCoL), a novel BC framework that ensures temporally consistent execution and fine-grained semantic grounding. It generates robust and smooth action execution trajectories through continuous co-learning across vision, language, and proprioceptive inputs (i.e., robot internal states). Meanwhile, we anchor language semantics to visuomotor representations by a bidirectional cross-attention to learn contextual information for action generation, successfully overcoming the problem of semantic-physical misalignment. Extensive experiments show that CCoL achieves an average 8.0% relative improvement across three simulation suites, with up to 19.2% relative gain in human-demonstrated bimanual insertion tasks. Real-world tests on a 7-DoF robot further confirm CCoL’s generalization under unseen and noisy object states.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Paper method -->
<section class="section hero">
  <div class="container is-max-widescreen">
    <div class="container is-max-widescreen">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        
        <h2 class="title is-3 has-text-centered">Methodology</h2>

        <h3 class="title is-4" style="margin-top: 2rem;">Motivation</h3>
        <div class="content has-text-justified">
          <p>
            Conventional Behavioral Cloning (BC) approaches suffer from two key multimodal grounding challenges that lead to inaccurate action cloning and incoherent execution.
          </p>
        </div>
        
        <div class="content">
          <ol style="margin-left: 2em;">
            
            <li>
              <h6 class="" style="margin-bottom: 0.5em;">Physical Discontinuities</h6>
              <p>
                Decoupled, per-step predictions fail to account for underlying motion dynamics. This leads to <strong>jerky trajectories</strong> (e.g., high-jerk robotic arm movements) and kinematically invalid transitions, ultimately causing failures in long-horizon tasks.
              </p>
            </li>

          <li style="margin-top: 1.5em;"> <h6 class="" style="margin-bottom: 0.5em;">Semantic-Physical Alignment</h6>
            <p>
              Static fusion methods fail to <strong>dynamically align language instructions</strong> with changing visuomotor states. For example, when executing "place the cup on the shelf," the robot must be able to shift its attention from the "cup" (during grasping) to the "shelf" (during placement).
            </p>
          </li>

  </ol>
      </div>

        <h3 class="title is-4" style="margin-top: 2rem;">Overall Architecture</h3>
        <div class="content has-text-justified">
          <p>
            To address these challenges, we present <strong>Continuous Vision-Language-Action Co-Learning with Semantic-Physical Alignment (CCoL)</strong>. Our framework introduces two novel components: <em>Multimodal Continuous Co-Learning (MCC) </em> and <em>Cross-modal Semantic-Physical Alignment (CSA)</em>. MCC leverages dynamic proprioceptive modeling to capture temporal evolution and maps multimodal representation into a shared latent space. CSA ensures stepwise synchronization of semantic information across modalities at each step. These components are built upon context-aware representation learning, which encodes fundamental multimodal inputs, and the fused enriched representations are used to generate contextually relevant and physically feasible action sequences.
          </p>
        </div>
        
        <div class="has-text-centered" style="margin-top: 1.5rem; margin-bottom: 1.5rem;">
          <img src="static/images/models.jpg" alt="Overview of the CCoL framework" style="width:100%; max-width:900px; border: 1px solid #eee;">
          <p class="is-size-6 has-text-grey" style="margin-top: 0.5rem;">
            Fig. 1: Overview of the CCoL framework. MCC leverages dynamic proprioceptive modeling to capture temporal evolution and maps multimodal inputs into a shared latent space <span style="color: purple;">(purple frame)</span>. CSA synchronizes stepwise semantic information across modalities <span style="color: red;">(red frame)</span> to enable the generation of contextually and physically feasible action sequences.
          </p>
        </div>

       <div class="content">
          <ol style="margin-left: 2em;">
            
            <li>
              <h6 class=" " style="margin-bottom: 0.5em;">Multimodal Continuous Co-Learning (MCC)</h6>
              <div class="content has-text-justified">
                <p>
                  To solve <em>physical discontinuities</em>, MCC introduces Neural Ordinary Differential Equations (NeuralODEs). Instead of modeling discrete, fragmented states, MCC captures the continuous evolution of proprioceptive embeddings by solving an initial value problem defined by a differential equation.
                </p>
                <p>
                  This approach provides temporally consistent representations that mitigate fragmentation and discontinuities found in conventional encoders. This directly results in the robust and smooth action trajectories seen in our experiments.
                </p>
              </div>
            </li>

            <li style="margin-top: 1.5em;">
              <h6 class=" " style="margin-bottom: 0.5em;">Cross-modal Semantic-Physical Alignment (CSA)</h6>
              <div class="content has-text-justified">
                <p>
                  To solve <em>semantic-physical misalignment</em>, CSA introduces a bidirectional cross-attention mechanism. This mechanism dynamically anchors high-level linguistic concepts (e.g., "cube", "socket") to the robot's low-level visuomotor representations at each specific timestep.
                </p>
              </div>

              <div class="has-text-centered" style="margin-top: 1.5rem; margin-bottom: 1.5rem;">
                <img src="static/images/attentivemap.jpg" alt="Attentive Attribute Map from CSA" style="width:100%; max-width:700px; border: 1px solid #eee;">
                <p class="is-size-6 has-text-grey" style="margin-top: 0.5rem;">
                  <strong>Fig. 2:</strong> Illustration of attentive attribute map. Observations, task goals (text instructions), and robot proprioceptive data are tokenized into multimodal embeddings. 
                </p>
              </div>
              <div class="content has-text-justified">
                <p>
                  Attention scores are computed separately for noun words (object-focused) and verb phrases (action-focused), as well as proprioceptive states, producing heatmaps that highlight relevant visual regions and trajectory features. Summing the heatmaps across layers generates attentive attribution maps, ensuring semantic grounding in visuomotor control. The fused features are calculated by attending from language to the visuomotor context, and vice-versa, ensuring a precise semantic-to-physical correspondence at each step of the task.
                </p>
              </div>
            </li>

          </ol>
        </div>
        
      </div> </div> </div>
</section>
<!-- End paper method -->

<section class="section hero" id="videos">
  <div class="container is-max-widescreen">

    <h2 class="title is-3 has-text-centered">Videos</h2>
    <p class="subtitle is-5 has-text-centered">
      Watch CCoL in action. We show results in simulation and on a real 7-DoF robot.
    </p>

    <div class="tabs is-centered is-boxed is-medium" id="video-tabs">
      <ul>
        <li class="is-active" data-tab="simulation-tab">
          <a>
            <span class="icon is-small"><i class="fas fa-desktop"></i></span>
            <span>Simulation</span>
          </a>
        </li>
        <li data-tab="real-world-tab">
          <a>
            <span class="icon is-small"><i class="fas fa-robot"></i></span>
            <span>Real-World (3 Videos)</span>
          </a>
        </li>
      </ul>
    </div>

    <div class="tabs-content-container">

      <div class="tab-content is-active" id="simulation-tab-content">
        <div class="video-item">
          <div style="text-align: center;"><h3>CCoL vs Baselines in Simulation</h3></div>
          <div class="video-embed">
            <iframe src="https://www.youtube.com/embed/kj3bvXrMllc" title="Simulator Results" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
          </div>
        </div>
      </div>

      <div class="tab-content" id="real-world-tab-content" style="display: none;">
        <div class="video-grid-3-col">
          <div class="video-item">
            <div style="text-align: center;"><h3>Real-World: Pen Lifting</h3></div>
            <div class="video-embed">
              <iframe src="https://www.youtube.com/embed/8Q2mN1Rc6yk" title="Real-World Scene 1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
            </div>
          </div>
          <div class="video-item">
            <div style="text-align: center;"><h3>Real-World: Cube Sliding</h3></div>
            <div class="video-embed">
              <iframe src="https://www.youtube.com/embed/1AIMAiQhhKQ" title="Real-World Scene 2" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
            </div>
          </div>
          <div class="video-item">
            <div style="text-align: center;"><h3>Real-World: Cubes Placement</h3></div>
            <div class="video-embed">
              <iframe src="https://www.youtube.com/embed/Tz91ZEMz_gU" title="Real-World Scene 3" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
            </div>
          </div>
        </div> </div> 
      </div> </div> </section>


<!-- <div class="content-section" id="videos">
    <h2>Videos</h2>
    
    <div class="video-grid-2-col">
        
        <div class="video-item">
            <div style="text-align: center;"><h3>Simulator Results</h3></div>
            <div class="video-embed">
                <iframe src="https://www.youtube.com/embed/kj3bvXrMllc" title="Simulator Results" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
            </div>
        </div>

        <div class="video-item">
            <div style="text-align: center;"><h3>Data Collection Process</h3></div>
            <div class="video-embed">
                <iframe src="https://www.youtube.com/embed/LCTqXsali-8" title="Data Collection Process" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
            </div>
        </div>
    </div> 
    
    <div class="video-grid-3-col">
        
        <div class="video-item">
            <div style="text-align: center;"><h3>Real-World Scene 1</h3></div>
            <div class="video-embed">
                <iframe src="https://www.youtube.com/embed/8Q2mN1Rc6yk" title="Real-World Scene 1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
            </div>
        </div>

        <div class="video-item">
            <div style="text-align: center;"><h3>Real-World Scene 2</h3></div>
            <div class="video-embed">
                <iframe src="https://www.youtube.com/embed/1AIMAiQhhKQ" title="Real-World Scene 2" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
            </div>
        </div>
        
        <div class="video-item">
            <div style="text-align: center;"><h3>Real-World Scene 3</h3></div>
            <div class="video-embed">
                <iframe src="https://www.youtube.com/embed/Tz91ZEMz_gU" title="Real-World Scene 3" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
            </div>
        </div>

    </div> 
</div> -->
    
<!-- Teaser video-->
<!-- <section class="hero teaser">
  <div class="container is-max-widescreen">
    <div class="hero-body"> -->
      <!-- TODO: Replace with your teaser video -->
      <!-- <video poster="" id="tree" autoplay controls muted loop height="100%" preload="metadata"> -->
        <!-- TODO: Add your video file path here -->
        <!-- <source src="static/videos/banner_video.mp4" type="video/mp4"> -->
      <!-- </video> -->
      <!-- TODO: Replace with your video description -->
      <!-- <h2 class="subtitle has-text-centered">
        Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus. 
      </h2>
    </div>
  </div>
</section> -->
<!-- End teaser video -->




<!-- Image carousel -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
         <img src="static/images/carousel1.jpg" alt="First research result visualization" loading="lazy"/> -->
        <!-- TODO: Replace with description of this result -->
        <!-- <h2 class="subtitle has-text-centered">
          First image description.
        </h2>
      </div>
      <div class="item"> -->
        <!-- <img src="static/images/carousel2.jpg" alt="Second research result visualization" loading="lazy"/>
        <h2 class="subtitle has-text-centered">
          Second image description.
        </h2>
      </div>
      <div class="item"> -->
        <!-- <img src="static/images/carousel3.jpg" alt="Third research result visualization" loading="lazy"/>
        <h2 class="subtitle has-text-centered">
         Third image description.
       </h2>
     </div>
     <div class="item">
      <img src="static/images/carousel4.jpg" alt="Fourth research result visualization" loading="lazy"/>
      <h2 class="subtitle has-text-centered">
        Fourth image description.
      </h2>
    </div>
  </div>
</div>
</div>
</section>  -->
<!-- End image carousel -->




<!-- Youtube video -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container"> -->
      <!-- Paper video. -->
      <!-- <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video"> -->
            <!-- TODO: Replace with your YouTube video ID -->
            <!-- <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End youtube video -->


<!-- Video carousel -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Videos</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" controls muted loop height="100%" preload="metadata">
            <source src="static/videos/carousel1.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" controls muted loop height="100%" preload="metadata">
            <source src="static/videos/carousel2.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" controls muted loop height="100%" preload="metadata">
            <source src="static/videos/carousel3.mp4" type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End video carousel -->






<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2> -->

      <!-- TODO: Replace with your poster PDF -->
      <!-- <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section> -->
<!--End paper poster -->



<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-widescreen content">
      <div class="bibtex-header">
        <h2 class="title">BibTeX</h2>
        <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
          <i class="fas fa-copy"></i>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <pre id="bibtex-code"><code>@article{YourPaperKey2024,
  title={Your Paper Title Here},
  author={First Author and Second Author and Third Author},
  journal={Conference/Journal Name},
  year={2024},
  url={https://your-domain.com/your-project-page}
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
